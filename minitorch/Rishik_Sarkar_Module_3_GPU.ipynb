{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qufLzTGmzs1K"
   },
   "source": [
    "## Step 3: Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Up_INtE4HJ75",
    "outputId": "94fb670e-e29e-4b23-f4d3-2de7bc76fd6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.12.7, pytest-8.3.2, pluggy-1.5.0\n",
      "rootdir: /content/mod3-RishikSarkar\n",
      "configfile: pyproject.toml\n",
      "plugins: hypothesis-6.54.0, env-1.1.4\n",
      "collected 117 items / 66 deselected / 51 selected                                                  \u001b[0m\n",
      "\n",
      "tests/test_tensor_general.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m================================ \u001b[32m\u001b[1m51 passed\u001b[0m, \u001b[33m66 deselected\u001b[0m\u001b[32m in 57.93s\u001b[0m\u001b[32m ================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 -m pytest tests/ -m task3_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abx-Z1x0eBAo",
    "outputId": "b8247ee4-2521-4cef-ad05-ccc9982030d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.12.7, pytest-8.3.2, pluggy-1.5.0\n",
      "rootdir: /content/mod3-RishikSarkar\n",
      "configfile: pyproject.toml\n",
      "plugins: hypothesis-6.54.0, env-1.1.4\n",
      "collected 117 items / 115 deselected / 2 selected                                                  \u001b[0m\n",
      "\n",
      "tests/test_tensor_general.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                              [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m================================ \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m115 deselected\u001b[0m\u001b[32m in 18.79s\u001b[0m\u001b[32m ================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 -m pytest tests/ -m task3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHWifxkFeEVA",
    "outputId": "71140af7-058a-4596-c98c-777d0a72d6eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP\n",
      " \n",
      "================================================================================\n",
      " Parallel Accelerator Optimizing:  Function tensor_map.<locals>._map, \n",
      "/content/mod3-RishikSarkar/minitorch/fast_ops.py (163)  \n",
      "================================================================================\n",
      "\n",
      "\n",
      "Parallel loop listing for  Function tensor_map.<locals>._map, /content/mod3-RishikSarkar/minitorch/fast_ops.py (163) \n",
      "-----------------------------------------------------------------------------|loop #ID\n",
      "    def _map(                                                                | \n",
      "        out: Storage,                                                        | \n",
      "        out_shape: Shape,                                                    | \n",
      "        out_strides: Strides,                                                | \n",
      "        in_storage: Storage,                                                 | \n",
      "        in_shape: Shape,                                                     | \n",
      "        in_strides: Strides,                                                 | \n",
      "    ) -> None:                                                               | \n",
      "        # Optimized path for identical shapes and strides                    | \n",
      "        out_size = np.prod(out_shape)----------------------------------------| #2\n",
      "        if np.array_equal(in_shape, out_shape) and np.array_equal(           | \n",
      "            in_strides, out_strides                                          | \n",
      "        ):                                                                   | \n",
      "            for i in prange(out_size):---------------------------------------| #3\n",
      "                out[i] = fn(in_storage[i])                                   | \n",
      "        else:                                                                | \n",
      "            for i in prange(out_size):---------------------------------------| #4\n",
      "                out_index = np.zeros(len(out_shape), dtype=np.int32)---------| #0\n",
      "                in_index = np.zeros(len(in_shape), dtype=np.int32)-----------| #1\n",
      "                                                                             | \n",
      "                to_index(i, out_shape, out_index)                            | \n",
      "                broadcast_index(out_index, out_shape, in_shape, in_index)    | \n",
      "                                                                             | \n",
      "                position = index_to_position(in_index, in_strides)           | \n",
      "                out[i] = fn(in_storage[position])                            | \n",
      "--------------------------------- Fusing loops ---------------------------------\n",
      "Attempting fusion of parallel loops (combines loops with similar properties)...\n",
      "Following the attempted fusion of parallel for-loops there are 5 parallel for-\n",
      "loop(s) (originating from loops labelled: #2, #3, #4, #0, #1).\n",
      "--------------------------------------------------------------------------------\n",
      "---------------------------- Optimising loop nests -----------------------------\n",
      "Attempting loop nest rewrites (optimising for the largest parallel loops)...\n",
      " \n",
      "+--4 is a parallel loop\n",
      "   +--0 --> rewritten as a serial loop\n",
      "   +--1 --> rewritten as a serial loop\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------- Before Optimisation ------------------------------\n",
      "Parallel region 0:\n",
      "+--4 (parallel)\n",
      "   +--0 (parallel)\n",
      "   +--1 (parallel)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------ After Optimisation ------------------------------\n",
      "Parallel region 0:\n",
      "+--4 (parallel)\n",
      "   +--0 (serial)\n",
      "   +--1 (serial)\n",
      "\n",
      "\n",
      " \n",
      "Parallel region 0 (loop #4) had 0 loop(s) fused and 2 loop(s) serialized as part\n",
      " of the larger parallel loop (#4).\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      " \n",
      "---------------------------Loop invariant code motion---------------------------\n",
      "Allocation hoisting:\n",
      "The memory allocation derived from the instruction at \n",
      "/content/mod3-RishikSarkar/minitorch/fast_ops.py (180) is hoisted out of the \n",
      "parallel loop labelled #4 (it will be performed before the loop is executed and \n",
      "reused inside the loop):\n",
      "   Allocation:: out_index = np.zeros(len(out_shape), dtype=np.int32)\n",
      "    - numpy.empty() is used for the allocation.\n",
      "The memory allocation derived from the instruction at \n",
      "/content/mod3-RishikSarkar/minitorch/fast_ops.py (181) is hoisted out of the \n",
      "parallel loop labelled #4 (it will be performed before the loop is executed and \n",
      "reused inside the loop):\n",
      "   Allocation:: in_index = np.zeros(len(in_shape), dtype=np.int32)\n",
      "    - numpy.empty() is used for the allocation.\n",
      "None\n",
      "ZIP\n",
      " \n",
      "================================================================================\n",
      " Parallel Accelerator Optimizing:  Function tensor_zip.<locals>._zip, \n",
      "/content/mod3-RishikSarkar/minitorch/fast_ops.py (215)  \n",
      "================================================================================\n",
      "\n",
      "\n",
      "Parallel loop listing for  Function tensor_zip.<locals>._zip, /content/mod3-RishikSarkar/minitorch/fast_ops.py (215) \n",
      "---------------------------------------------------------------------------|loop #ID\n",
      "    def _zip(                                                              | \n",
      "        out: Storage,                                                      | \n",
      "        out_shape: Shape,                                                  | \n",
      "        out_strides: Strides,                                              | \n",
      "        a_storage: Storage,                                                | \n",
      "        a_shape: Shape,                                                    | \n",
      "        a_strides: Strides,                                                | \n",
      "        b_storage: Storage,                                                | \n",
      "        b_shape: Shape,                                                    | \n",
      "        b_strides: Strides,                                                | \n",
      "    ) -> None:                                                             | \n",
      "        # Optimized path if all shapes and strides match                   | \n",
      "        if (                                                               | \n",
      "            np.array_equal(a_strides, out_strides)                         | \n",
      "            and np.array_equal(b_strides, out_strides)                     | \n",
      "            and np.array_equal(a_shape, out_shape)                         | \n",
      "            and np.array_equal(b_shape, out_shape)                         | \n",
      "        ):                                                                 | \n",
      "            for i in prange(len(out)):-------------------------------------| #8\n",
      "                out[i] = fn(a_storage[i], b_storage[i])                    | \n",
      "        else:                                                              | \n",
      "            for i in prange(len(out)):-------------------------------------| #9\n",
      "                a_index = np.zeros(len(a_shape), dtype=np.int32)-----------| #5\n",
      "                b_index = np.zeros(len(b_shape), dtype=np.int32)-----------| #6\n",
      "                out_index = np.zeros(len(out_shape), dtype=np.int32)-------| #7\n",
      "                                                                           | \n",
      "                to_index(i, out_shape, out_index)                          | \n",
      "                broadcast_index(out_index, out_shape, a_shape, a_index)    | \n",
      "                broadcast_index(out_index, out_shape, b_shape, b_index)    | \n",
      "                                                                           | \n",
      "                a_pos = index_to_position(a_index, a_strides)              | \n",
      "                b_pos = index_to_position(b_index, b_strides)              | \n",
      "                                                                           | \n",
      "                out[i] = fn(a_storage[a_pos], b_storage[b_pos])            | \n",
      "--------------------------------- Fusing loops ---------------------------------\n",
      "Attempting fusion of parallel loops (combines loops with similar properties)...\n",
      "Following the attempted fusion of parallel for-loops there are 5 parallel for-\n",
      "loop(s) (originating from loops labelled: #8, #9, #5, #6, #7).\n",
      "--------------------------------------------------------------------------------\n",
      "---------------------------- Optimising loop nests -----------------------------\n",
      "Attempting loop nest rewrites (optimising for the largest parallel loops)...\n",
      " \n",
      "+--9 is a parallel loop\n",
      "   +--5 --> rewritten as a serial loop\n",
      "   +--6 --> rewritten as a serial loop\n",
      "   +--7 --> rewritten as a serial loop\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------- Before Optimisation ------------------------------\n",
      "Parallel region 0:\n",
      "+--9 (parallel)\n",
      "   +--5 (parallel)\n",
      "   +--6 (parallel)\n",
      "   +--7 (parallel)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------ After Optimisation ------------------------------\n",
      "Parallel region 0:\n",
      "+--9 (parallel)\n",
      "   +--5 (serial)\n",
      "   +--6 (serial)\n",
      "   +--7 (serial)\n",
      "\n",
      "\n",
      " \n",
      "Parallel region 0 (loop #9) had 0 loop(s) fused and 3 loop(s) serialized as part\n",
      " of the larger parallel loop (#9).\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      " \n",
      "---------------------------Loop invariant code motion---------------------------\n",
      "Allocation hoisting:\n",
      "The memory allocation derived from the instruction at \n",
      "/content/mod3-RishikSarkar/minitorch/fast_ops.py (237) is hoisted out of the \n",
      "parallel loop labelled #9 (it will be performed before the loop is executed and \n",
      "reused inside the loop):\n",
      "   Allocation:: a_index = np.zeros(len(a_shape), dtype=np.int32)\n",
      "    - numpy.empty() is used for the allocation.\n",
      "The memory allocation derived from the instruction at \n",
      "/content/mod3-RishikSarkar/minitorch/fast_ops.py (238) is hoisted out of the \n",
      "parallel loop labelled #9 (it will be performed before the loop is executed and \n",
      "reused inside the loop):\n",
      "   Allocation:: b_index = np.zeros(len(b_shape), dtype=np.int32)\n",
      "    - numpy.empty() is used for the allocation.\n",
      "The memory allocation derived from the instruction at \n",
      "/content/mod3-RishikSarkar/minitorch/fast_ops.py (239) is hoisted out of the \n",
      "parallel loop labelled #9 (it will be performed before the loop is executed and \n",
      "reused inside the loop):\n",
      "   Allocation:: out_index = np.zeros(len(out_shape), dtype=np.int32)\n",
      "    - numpy.empty() is used for the allocation.\n",
      "None\n",
      "REDUCE\n",
      " \n",
      "================================================================================\n",
      " Parallel Accelerator Optimizing:  Function tensor_reduce.<locals>._reduce, \n",
      "/content/mod3-RishikSarkar/minitorch/fast_ops.py (274)  \n",
      "================================================================================\n",
      "\n",
      "\n",
      "Parallel loop listing for  Function tensor_reduce.<locals>._reduce, /content/mod3-RishikSarkar/minitorch/fast_ops.py (274) \n",
      "-----------------------------------------------------------------------|loop #ID\n",
      "    def _reduce(                                                       | \n",
      "        out: Storage,                                                  | \n",
      "        out_shape: Shape,                                              | \n",
      "        out_strides: Strides,                                          | \n",
      "        a_storage: Storage,                                            | \n",
      "        a_shape: Shape,                                                | \n",
      "        a_strides: Strides,                                            | \n",
      "        reduce_dim: int,                                               | \n",
      "    ) -> None:                                                         | \n",
      "        dim_len = a_shape[reduce_dim]                                  | \n",
      "        start = out[0]                                                 | \n",
      "                                                                       | \n",
      "        for i in prange(np.prod(out_shape)):---------------------------| #13, 12\n",
      "            out_index = np.zeros(len(out_shape), dtype=np.int32)-------| #10\n",
      "            a_index = np.zeros(len(a_shape), dtype=np.int32)-----------| #11\n",
      "                                                                       | \n",
      "            to_index(i, out_shape, out_index)                          | \n",
      "                                                                       | \n",
      "            # Reduce across dimension                                  | \n",
      "            acc = start                                                | \n",
      "            for j in range(dim_len):                                   | \n",
      "                for k in range(len(out_shape)):                        | \n",
      "                    a_index[k] = out_index[k]                          | \n",
      "                a_index[reduce_dim] = j                                | \n",
      "                a_pos = index_to_position(a_index, a_strides)          | \n",
      "                acc = fn(acc, a_storage[a_pos])                        | \n",
      "                                                                       | \n",
      "            out[i] = acc                                               | \n",
      "--------------------------------- Fusing loops ---------------------------------\n",
      "Attempting fusion of parallel loops (combines loops with similar properties)...\n",
      "Following the attempted fusion of parallel for-loops there are 4 parallel for-\n",
      "loop(s) (originating from loops labelled: #12, #13, #10, #11).\n",
      "--------------------------------------------------------------------------------\n",
      "---------------------------- Optimising loop nests -----------------------------\n",
      "Attempting loop nest rewrites (optimising for the largest parallel loops)...\n",
      " \n",
      "+--13 is a parallel loop\n",
      "   +--10 --> rewritten as a serial loop\n",
      "   +--11 --> rewritten as a serial loop\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------- Before Optimisation ------------------------------\n",
      "Parallel region 0:\n",
      "+--13 (parallel)\n",
      "   +--10 (parallel)\n",
      "   +--11 (parallel)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------ After Optimisation ------------------------------\n",
      "Parallel region 0:\n",
      "+--13 (parallel)\n",
      "   +--10 (serial)\n",
      "   +--11 (serial)\n",
      "\n",
      "\n",
      " \n",
      "Parallel region 0 (loop #13) had 0 loop(s) fused and 2 loop(s) serialized as \n",
      "part of the larger parallel loop (#13).\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      " \n",
      "---------------------------Loop invariant code motion---------------------------\n",
      "Allocation hoisting:\n",
      "The memory allocation derived from the instruction at \n",
      "/content/mod3-RishikSarkar/minitorch/fast_ops.py (287) is hoisted out of the \n",
      "parallel loop labelled #13 (it will be performed before the loop is executed and\n",
      " reused inside the loop):\n",
      "   Allocation:: out_index = np.zeros(len(out_shape), dtype=np.int32)\n",
      "    - numpy.empty() is used for the allocation.\n",
      "The memory allocation derived from the instruction at \n",
      "/content/mod3-RishikSarkar/minitorch/fast_ops.py (288) is hoisted out of the \n",
      "parallel loop labelled #13 (it will be performed before the loop is executed and\n",
      " reused inside the loop):\n",
      "   Allocation:: a_index = np.zeros(len(a_shape), dtype=np.int32)\n",
      "    - numpy.empty() is used for the allocation.\n",
      "None\n",
      "MATRIX MULTIPLY\n",
      " \n",
      "================================================================================\n",
      " Parallel Accelerator Optimizing:  Function _tensor_matrix_multiply, \n",
      "/content/mod3-RishikSarkar/minitorch/fast_ops.py (306)  \n",
      "================================================================================\n",
      "\n",
      "\n",
      "Parallel loop listing for  Function _tensor_matrix_multiply, /content/mod3-RishikSarkar/minitorch/fast_ops.py (306) \n",
      "------------------------------------------------------------------------|loop #ID\n",
      "def _tensor_matrix_multiply(                                            | \n",
      "    out: Storage,                                                       | \n",
      "    out_shape: Shape,                                                   | \n",
      "    out_strides: Strides,                                               | \n",
      "    a_storage: Storage,                                                 | \n",
      "    a_shape: Shape,                                                     | \n",
      "    a_strides: Strides,                                                 | \n",
      "    b_storage: Storage,                                                 | \n",
      "    b_shape: Shape,                                                     | \n",
      "    b_strides: Strides,                                                 | \n",
      ") -> None:                                                              | \n",
      "    \"\"\"NUMBA tensor matrix multiply function.                           | \n",
      "                                                                        | \n",
      "    Should work for any tensor shapes that broadcast as long as         | \n",
      "                                                                        | \n",
      "    ```                                                                 | \n",
      "    assert a_shape[-1] == b_shape[-2]                                   | \n",
      "    ```                                                                 | \n",
      "                                                                        | \n",
      "    Optimizations:                                                      | \n",
      "                                                                        | \n",
      "    * Outer loop in parallel                                            | \n",
      "    * No index buffers or function calls                                | \n",
      "    * Inner loop should have no global writes, 1 multiply.              | \n",
      "                                                                        | \n",
      "                                                                        | \n",
      "    Args:                                                               | \n",
      "    ----                                                                | \n",
      "        out (Storage): storage for `out` tensor                         | \n",
      "        out_shape (Shape): shape for `out` tensor                       | \n",
      "        out_strides (Strides): strides for `out` tensor                 | \n",
      "        a_storage (Storage): storage for `a` tensor                     | \n",
      "        a_shape (Shape): shape for `a` tensor                           | \n",
      "        a_strides (Strides): strides for `a` tensor                     | \n",
      "        b_storage (Storage): storage for `b` tensor                     | \n",
      "        b_shape (Shape): shape for `b` tensor                           | \n",
      "        b_strides (Strides): strides for `b` tensor                     | \n",
      "                                                                        | \n",
      "    Returns:                                                            | \n",
      "    -------                                                             | \n",
      "        None : Fills in `out`                                           | \n",
      "                                                                        | \n",
      "    \"\"\"                                                                 | \n",
      "    assert a_shape[-1] == b_shape[-2], \"Shapes do not match!\"           | \n",
      "                                                                        | \n",
      "    a_batch_stride = a_strides[0] if a_shape[0] > 1 else 0              | \n",
      "    b_batch_stride = b_strides[0] if b_shape[0] > 1 else 0              | \n",
      "                                                                        | \n",
      "    for batch in prange(out_shape[0]):----------------------------------| #14\n",
      "        for row in range(out_shape[-2]):                                | \n",
      "            for col in range(out_shape[-1]):                            | \n",
      "                # Calculate base positions once                         | \n",
      "                a_pos = batch * a_batch_stride + row * a_strides[-2]    | \n",
      "                b_pos = batch * b_batch_stride + col * b_strides[-1]    | \n",
      "                                                                        | \n",
      "                accumulator = 0.0                                       | \n",
      "                for k in range(a_shape[-1]):                            | \n",
      "                    accumulator += (                                    | \n",
      "                        a_storage[a_pos + k * a_strides[-1]]            | \n",
      "                        * b_storage[b_pos + k * b_strides[-2]]          | \n",
      "                    )                                                   | \n",
      "                                                                        | \n",
      "                out_pos = (                                             | \n",
      "                    batch * out_strides[0]                              | \n",
      "                    + row * out_strides[-2]                             | \n",
      "                    + col * out_strides[-1]                             | \n",
      "                )                                                       | \n",
      "                out[out_pos] = accumulator                              | \n",
      "--------------------------------- Fusing loops ---------------------------------\n",
      "Attempting fusion of parallel loops (combines loops with similar properties)...\n",
      "Following the attempted fusion of parallel for-loops there are 1 parallel for-\n",
      "loop(s) (originating from loops labelled: #14).\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------- Before Optimisation ------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------ After Optimisation ------------------------------\n",
      "Parallel structure is already optimal.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      " \n",
      "---------------------------Loop invariant code motion---------------------------\n",
      "Allocation hoisting:\n",
      "No allocation hoisting found\n",
      "None\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 project/parallel_check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNlO8mLZeB1j",
    "outputId": "1634cd6b-42eb-4dcc-8b1b-1dc5b1e0a2f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.12.7, pytest-8.3.2, pluggy-1.5.0\n",
      "rootdir: /content/mod3-RishikSarkar\n",
      "configfile: pyproject.toml\n",
      "plugins: hypothesis-6.54.0, env-1.1.4\n",
      "collected 117 items / 60 deselected / 57 selected                                                  \u001b[0m\n",
      "\n",
      "tests/test_tensor_general.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m       [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m========================================= warnings summary =========================================\u001b[0m\n",
      "tests/test_tensor_general.py: 16 warnings\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py: 4268 warnings\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py: 12 warnings\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_one_args[cuda-fn0]\n",
      "tests/test_tensor_general.py::test_one_args[cuda-fn1]\n",
      "tests/test_tensor_general.py::test_one_derivative[cuda-fn0]\n",
      "tests/test_tensor_general.py::test_one_derivative[cuda-fn6]\n",
      "tests/test_tensor_general.py::test_one_derivative[cuda-fn10]\n",
      "tests/test_tensor_general.py::test_sum_practice2\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 3 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_one_derivative[cuda-fn0]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_one_derivative[cuda-fn0]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 6 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_one_derivative[cuda-fn0]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 9 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_one_derivative[cuda-fn0]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 12 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_one_derivative[cuda-fn0]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 18 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_one_derivative[cuda-fn1]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 8 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_one_derivative[cuda-fn1]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 27 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_sum_practice_other_dims\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 16 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m=================== \u001b[32m57 passed\u001b[0m, \u001b[33m\u001b[1m60 deselected\u001b[0m, \u001b[33m\u001b[1m4310 warnings\u001b[0m\u001b[33m in 195.66s (0:03:15)\u001b[0m\u001b[33m ===================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 -m pytest tests/ -m task3_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IhKhfU9teCoY",
    "outputId": "d029a1df-672c-4e28-9ab5-25c6e862125a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.12.7, pytest-8.3.2, pluggy-1.5.0\n",
      "rootdir: /content/mod3-RishikSarkar\n",
      "configfile: pyproject.toml\n",
      "plugins: hypothesis-6.54.0, env-1.1.4\n",
      "collected 117 items / 110 deselected / 7 selected                                                  \u001b[0m\n",
      "\n",
      "tests/test_tensor_general.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                         [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m========================================= warnings summary =========================================\u001b[0m\n",
      "tests/test_tensor_general.py::test_mul_practice1\n",
      "tests/test_tensor_general.py::test_mul_practice3\n",
      "tests/test_tensor_general.py::test_mul_practice3\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py: 111 warnings\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_mul_practice4\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 35 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_mul_practice4\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_mul_practice5\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 8 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 12 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 3 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 27 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 24 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 36 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 16 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 18 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 48 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 5 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "tests/test_tensor_general.py::test_bmm[cuda]\n",
      "  /usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 6 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "    warn(NumbaPerformanceWarning(msg))\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m========================= \u001b[32m7 passed\u001b[0m, \u001b[33m\u001b[1m110 deselected\u001b[0m, \u001b[33m\u001b[1m139 warnings\u001b[0m\u001b[33m in 12.57s\u001b[0m\u001b[33m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 -m pytest tests/ -m task3_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzU4WsUCegCQ",
    "outputId": "7b423524-c1d9-43ba-ef4f-57e714b2cf1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 16 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "Running size 64\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 8 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "{'fast': np.float64(0.004188378651936849), 'gpu': np.float64(0.010809659957885742)}\n",
      "Running size 128\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "{'fast': np.float64(0.019573688507080078), 'gpu': np.float64(0.019053300221761067)}\n",
      "Running size 256\n",
      "{'fast': np.float64(0.11205275853474934), 'gpu': np.float64(0.060024261474609375)}\n",
      "Running size 512\n",
      "{'fast': np.float64(1.030336618423462), 'gpu': np.float64(0.23212432861328125)}\n",
      "Running size 1024\n",
      "{'fast': np.float64(12.476243734359741), 'gpu': np.float64(1.0347240765889485)}\n",
      "\n",
      "Timing summary\n",
      "Size: 64\n",
      "    fast: 0.00419\n",
      "    gpu: 0.01081\n",
      "Size: 128\n",
      "    fast: 0.01957\n",
      "    gpu: 0.01905\n",
      "Size: 256\n",
      "    fast: 0.11205\n",
      "    gpu: 0.06002\n",
      "Size: 512\n",
      "    fast: 1.03034\n",
      "    gpu: 0.23212\n",
      "Size: 1024\n",
      "    fast: 12.47624\n",
      "    gpu: 1.03472\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 minitorch/timing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3ZWd_74e19Y"
   },
   "source": [
    "## Step 4: Model Training (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FY8Q4MqEzvGW",
    "outputId": "733ae10b-6b9b-4d2e-f233-6a11100491a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Loss 7.21856 | Correct   32 | Time 22.2749s\n",
      "Epoch   10 | Loss 5.32558 | Correct   43 | Time 0.1033s\n",
      "Epoch   20 | Loss 3.81465 | Correct   43 | Time 0.1166s\n",
      "Epoch   30 | Loss 3.00135 | Correct   44 | Time 0.0826s\n",
      "Epoch   40 | Loss 2.71256 | Correct   46 | Time 0.0824s\n",
      "Epoch   50 | Loss 2.05973 | Correct   46 | Time 0.0809s\n",
      "Epoch   60 | Loss 3.43272 | Correct   48 | Time 0.0846s\n",
      "Epoch   70 | Loss 3.04089 | Correct   48 | Time 0.0834s\n",
      "Epoch   80 | Loss 1.89759 | Correct   46 | Time 0.0856s\n",
      "Epoch   90 | Loss 1.88431 | Correct   48 | Time 0.0827s\n",
      "Epoch  100 | Loss 2.85775 | Correct   47 | Time 0.0825s\n",
      "Epoch  110 | Loss 1.80508 | Correct   48 | Time 0.0817s\n",
      "Epoch  120 | Loss 2.04550 | Correct   49 | Time 0.0821s\n",
      "Epoch  130 | Loss 2.40559 | Correct   48 | Time 0.0809s\n",
      "Epoch  140 | Loss 0.75429 | Correct   48 | Time 0.1424s\n",
      "Epoch  150 | Loss 0.44224 | Correct   49 | Time 0.1174s\n",
      "Epoch  160 | Loss 0.70657 | Correct   48 | Time 0.0819s\n",
      "Epoch  170 | Loss 0.90560 | Correct   48 | Time 0.0834s\n",
      "Epoch  180 | Loss 1.81533 | Correct   49 | Time 0.0856s\n",
      "Epoch  190 | Loss 3.33573 | Correct   49 | Time 0.0827s\n",
      "Epoch  200 | Loss 1.28498 | Correct   49 | Time 0.0832s\n",
      "Epoch  210 | Loss 0.76139 | Correct   49 | Time 0.0828s\n",
      "Epoch  220 | Loss 1.84151 | Correct   49 | Time 0.0826s\n",
      "Epoch  230 | Loss 1.08600 | Correct   49 | Time 0.0851s\n",
      "Epoch  240 | Loss 2.35033 | Correct   50 | Time 0.0853s\n",
      "Epoch  250 | Loss 1.45641 | Correct   49 | Time 0.0948s\n",
      "Epoch  260 | Loss 2.41817 | Correct   50 | Time 0.0830s\n",
      "Epoch  270 | Loss 1.27345 | Correct   50 | Time 0.0820s\n",
      "Epoch  280 | Loss 2.08741 | Correct   50 | Time 0.0951s\n",
      "Epoch  290 | Loss 1.88510 | Correct   50 | Time 0.1051s\n",
      "Epoch  300 | Loss 0.81144 | Correct   49 | Time 0.0853s\n",
      "Epoch  310 | Loss 1.64322 | Correct   50 | Time 0.0956s\n",
      "Epoch  320 | Loss 0.23205 | Correct   49 | Time 0.0821s\n",
      "Epoch  330 | Loss 2.35809 | Correct   50 | Time 0.0814s\n",
      "Epoch  340 | Loss 0.82929 | Correct   49 | Time 0.0815s\n",
      "Epoch  350 | Loss 0.91656 | Correct   50 | Time 0.0824s\n",
      "Epoch  360 | Loss 0.28191 | Correct   49 | Time 0.0820s\n",
      "Epoch  370 | Loss 0.86287 | Correct   49 | Time 0.0818s\n",
      "Epoch  380 | Loss 1.06964 | Correct   49 | Time 0.0832s\n",
      "Epoch  390 | Loss 0.02450 | Correct   50 | Time 0.0829s\n",
      "Epoch  400 | Loss 0.47279 | Correct   50 | Time 0.0823s\n",
      "Epoch  410 | Loss 0.80637 | Correct   49 | Time 0.1202s\n",
      "Epoch  420 | Loss 0.18363 | Correct   49 | Time 0.1467s\n",
      "Epoch  430 | Loss 1.23548 | Correct   49 | Time 0.0909s\n",
      "Epoch  440 | Loss 0.39607 | Correct   49 | Time 0.0801s\n",
      "Epoch  450 | Loss 0.96854 | Correct   49 | Time 0.0822s\n",
      "Epoch  460 | Loss 1.45020 | Correct   50 | Time 0.0825s\n",
      "Epoch  470 | Loss 0.50692 | Correct   49 | Time 0.0819s\n",
      "Epoch  480 | Loss 0.47598 | Correct   49 | Time 0.0853s\n",
      "Epoch  490 | Loss 0.22326 | Correct   50 | Time 0.0814s\n",
      "Epoch  499 | Loss 0.29172 | Correct   49 | Time 0.0802s\n",
      "\n",
      "Training Statistics:\n",
      "Total training time: 69.21s\n",
      "Average epoch time: 0.1384s\n",
      "Final accuracy: 98.0% (49/50 correct)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 project/run_fast_tensor.py --BACKEND cpu --HIDDEN 100 --DATASET xor --RATE 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mn4bqxAPfTJ1",
    "outputId": "12cc3f94-aa03-419d-99b9-ab00ad70f7d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Loss 9.12181 | Correct   28 | Time 21.5103s\n",
      "Epoch   10 | Loss 2.74563 | Correct   46 | Time 0.3104s\n",
      "Epoch   20 | Loss 3.76538 | Correct   45 | Time 0.1807s\n",
      "Epoch   30 | Loss 1.69206 | Correct   48 | Time 0.1776s\n",
      "Epoch   40 | Loss 2.29748 | Correct   43 | Time 0.1855s\n",
      "Epoch   50 | Loss 1.60171 | Correct   45 | Time 0.1879s\n",
      "Epoch   60 | Loss 1.70288 | Correct   47 | Time 0.1800s\n",
      "Epoch   70 | Loss 4.81925 | Correct   44 | Time 0.3631s\n",
      "Epoch   80 | Loss 0.77565 | Correct   49 | Time 0.1739s\n",
      "Epoch   90 | Loss 1.71730 | Correct   48 | Time 0.1741s\n",
      "Epoch  100 | Loss 1.11515 | Correct   49 | Time 0.1898s\n",
      "Epoch  110 | Loss 1.19783 | Correct   49 | Time 0.1776s\n",
      "Epoch  120 | Loss 1.13881 | Correct   49 | Time 0.1755s\n",
      "Epoch  130 | Loss 0.64782 | Correct   49 | Time 0.2472s\n",
      "Epoch  140 | Loss 2.10208 | Correct   47 | Time 0.1770s\n",
      "Epoch  150 | Loss 1.82883 | Correct   49 | Time 0.1777s\n",
      "Epoch  160 | Loss 1.51496 | Correct   50 | Time 0.1728s\n",
      "Epoch  170 | Loss 1.71655 | Correct   49 | Time 0.1912s\n",
      "Epoch  180 | Loss 1.36411 | Correct   49 | Time 0.1747s\n",
      "Epoch  190 | Loss 3.69900 | Correct   44 | Time 0.1768s\n",
      "Epoch  200 | Loss 1.20904 | Correct   48 | Time 0.4306s\n",
      "Epoch  210 | Loss 1.82997 | Correct   48 | Time 0.1724s\n",
      "Epoch  220 | Loss 1.25127 | Correct   48 | Time 0.1936s\n",
      "Epoch  230 | Loss 2.66129 | Correct   48 | Time 0.1754s\n",
      "Epoch  240 | Loss 0.27691 | Correct   49 | Time 0.1981s\n",
      "Epoch  250 | Loss 2.76606 | Correct   47 | Time 0.1738s\n",
      "Epoch  260 | Loss 0.16908 | Correct   49 | Time 0.3716s\n",
      "Epoch  270 | Loss 0.74632 | Correct   48 | Time 0.1914s\n",
      "Epoch  280 | Loss 0.68879 | Correct   49 | Time 0.1795s\n",
      "Epoch  290 | Loss 0.70391 | Correct   50 | Time 0.1769s\n",
      "Epoch  300 | Loss 0.72761 | Correct   49 | Time 0.1777s\n",
      "Epoch  310 | Loss 0.82416 | Correct   49 | Time 0.1892s\n",
      "Epoch  320 | Loss 0.30423 | Correct   48 | Time 0.3185s\n",
      "Epoch  330 | Loss 0.53566 | Correct   49 | Time 0.1774s\n",
      "Epoch  340 | Loss 1.48451 | Correct   49 | Time 0.1883s\n",
      "Epoch  350 | Loss 0.18256 | Correct   49 | Time 0.1771s\n",
      "Epoch  360 | Loss 1.50356 | Correct   49 | Time 0.1882s\n",
      "Epoch  370 | Loss 0.77627 | Correct   49 | Time 0.1785s\n",
      "Epoch  380 | Loss 0.88269 | Correct   49 | Time 0.1724s\n",
      "Epoch  390 | Loss 0.13936 | Correct   50 | Time 0.1787s\n",
      "Epoch  400 | Loss 1.69210 | Correct   49 | Time 0.1769s\n",
      "Epoch  410 | Loss 0.25552 | Correct   50 | Time 0.1882s\n",
      "Epoch  420 | Loss 1.66820 | Correct   49 | Time 0.1739s\n",
      "Epoch  430 | Loss 1.21889 | Correct   49 | Time 0.1750s\n",
      "Epoch  440 | Loss 0.07750 | Correct   49 | Time 0.1765s\n",
      "Epoch  450 | Loss 0.02701 | Correct   49 | Time 0.3021s\n",
      "Epoch  460 | Loss 0.91109 | Correct   49 | Time 0.1781s\n",
      "Epoch  470 | Loss 0.57807 | Correct   49 | Time 0.1738s\n",
      "Epoch  480 | Loss 0.30807 | Correct   49 | Time 0.1782s\n",
      "Epoch  490 | Loss 0.30463 | Correct   49 | Time 0.1896s\n",
      "Epoch  499 | Loss 0.09075 | Correct   49 | Time 0.1756s\n",
      "\n",
      "Training Statistics:\n",
      "Total training time: 121.57s\n",
      "Average epoch time: 0.2431s\n",
      "Final accuracy: 98.0% (49/50 correct)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 project/run_fast_tensor.py --BACKEND cpu --HIDDEN 200 --DATASET xor --RATE 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3HYPC57fVFu",
    "outputId": "0710ec1d-ab1d-4ab7-f904-eedabd63047e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Loss 5.14090 | Correct   42 | Time 22.1457s\n",
      "Epoch   10 | Loss 2.72324 | Correct   47 | Time 0.0848s\n",
      "Epoch   20 | Loss 1.10878 | Correct   48 | Time 0.0821s\n",
      "Epoch   30 | Loss 1.42843 | Correct   50 | Time 0.0878s\n",
      "Epoch   40 | Loss 0.62106 | Correct   50 | Time 0.0830s\n",
      "Epoch   50 | Loss 0.38313 | Correct   50 | Time 0.0820s\n",
      "Epoch   60 | Loss 0.93927 | Correct   50 | Time 0.0812s\n",
      "Epoch   70 | Loss 0.63442 | Correct   50 | Time 0.0841s\n",
      "Epoch   80 | Loss 0.11810 | Correct   50 | Time 0.0839s\n",
      "Epoch   90 | Loss 0.83397 | Correct   50 | Time 0.0902s\n",
      "Epoch  100 | Loss 0.28312 | Correct   50 | Time 0.0837s\n",
      "Epoch  110 | Loss 0.63046 | Correct   50 | Time 0.0819s\n",
      "Epoch  120 | Loss 0.56827 | Correct   50 | Time 0.0839s\n",
      "Epoch  130 | Loss 0.43606 | Correct   50 | Time 0.1790s\n",
      "Epoch  140 | Loss 0.20000 | Correct   50 | Time 0.0855s\n",
      "Epoch  150 | Loss 0.83091 | Correct   50 | Time 0.0855s\n",
      "Epoch  160 | Loss 0.68641 | Correct   50 | Time 0.0854s\n",
      "Epoch  170 | Loss 0.14442 | Correct   50 | Time 0.0819s\n",
      "Epoch  180 | Loss 0.79301 | Correct   50 | Time 0.0833s\n",
      "Epoch  190 | Loss 0.77080 | Correct   50 | Time 0.0813s\n",
      "Epoch  200 | Loss 0.61272 | Correct   50 | Time 0.0808s\n",
      "Epoch  210 | Loss 0.00478 | Correct   50 | Time 0.0833s\n",
      "Epoch  220 | Loss 0.10128 | Correct   50 | Time 0.0848s\n",
      "Epoch  230 | Loss 0.26765 | Correct   50 | Time 0.0829s\n",
      "Epoch  240 | Loss 0.29440 | Correct   50 | Time 0.0843s\n",
      "Epoch  250 | Loss 0.12794 | Correct   50 | Time 0.0821s\n",
      "Epoch  260 | Loss 0.76984 | Correct   50 | Time 0.2087s\n",
      "Epoch  270 | Loss 0.28361 | Correct   50 | Time 0.1596s\n",
      "Epoch  280 | Loss 0.27353 | Correct   50 | Time 0.0825s\n",
      "Epoch  290 | Loss 0.00231 | Correct   50 | Time 0.0846s\n",
      "Epoch  300 | Loss 0.12813 | Correct   50 | Time 0.1664s\n",
      "Epoch  310 | Loss 0.18254 | Correct   50 | Time 0.1901s\n",
      "Epoch  320 | Loss 0.57850 | Correct   50 | Time 0.0859s\n",
      "Epoch  330 | Loss 0.37608 | Correct   50 | Time 0.0817s\n",
      "Epoch  340 | Loss 0.23327 | Correct   50 | Time 0.0946s\n",
      "Epoch  350 | Loss 0.00447 | Correct   50 | Time 0.0834s\n",
      "Epoch  360 | Loss 0.03869 | Correct   50 | Time 0.0830s\n",
      "Epoch  370 | Loss 0.00019 | Correct   50 | Time 0.0833s\n",
      "Epoch  380 | Loss 0.41531 | Correct   50 | Time 0.1137s\n",
      "Epoch  390 | Loss 0.08752 | Correct   50 | Time 0.1198s\n",
      "Epoch  400 | Loss 0.02466 | Correct   50 | Time 0.0931s\n",
      "Epoch  410 | Loss 0.04711 | Correct   50 | Time 0.0804s\n",
      "Epoch  420 | Loss 0.01249 | Correct   50 | Time 0.0843s\n",
      "Epoch  430 | Loss 0.10796 | Correct   50 | Time 0.0808s\n",
      "Epoch  440 | Loss 0.00254 | Correct   50 | Time 0.0843s\n",
      "Epoch  450 | Loss 0.21190 | Correct   50 | Time 0.0848s\n",
      "Epoch  460 | Loss 0.42549 | Correct   50 | Time 0.1072s\n",
      "Epoch  470 | Loss 0.41604 | Correct   50 | Time 0.0811s\n",
      "Epoch  480 | Loss 0.30401 | Correct   50 | Time 0.0836s\n",
      "Epoch  490 | Loss 0.34243 | Correct   50 | Time 0.0862s\n",
      "Epoch  499 | Loss 0.28516 | Correct   50 | Time 0.0843s\n",
      "\n",
      "Training Statistics:\n",
      "Total training time: 69.64s\n",
      "Average epoch time: 0.1393s\n",
      "Final accuracy: 100.0% (50/50 correct)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 project/run_fast_tensor.py --BACKEND cpu --HIDDEN 100 --DATASET simple --RATE 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5j6delcfW5X",
    "outputId": "affada0b-6f50-4133-b6fa-743ac9821414"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Loss 4.21965 | Correct   48 | Time 22.3279s\n",
      "Epoch   10 | Loss 0.80876 | Correct   48 | Time 0.1752s\n",
      "Epoch   20 | Loss 1.64110 | Correct   47 | Time 0.1724s\n",
      "Epoch   30 | Loss 1.07481 | Correct   49 | Time 0.1707s\n",
      "Epoch   40 | Loss 0.62205 | Correct   49 | Time 0.1791s\n",
      "Epoch   50 | Loss 0.31752 | Correct   49 | Time 0.1749s\n",
      "Epoch   60 | Loss 1.32554 | Correct   49 | Time 0.2915s\n",
      "Epoch   70 | Loss 1.47882 | Correct   49 | Time 0.1738s\n",
      "Epoch   80 | Loss 0.67100 | Correct   49 | Time 0.1870s\n",
      "Epoch   90 | Loss 0.47517 | Correct   49 | Time 0.1778s\n",
      "Epoch  100 | Loss 0.43606 | Correct   50 | Time 0.1733s\n",
      "Epoch  110 | Loss 0.18810 | Correct   49 | Time 0.1783s\n",
      "Epoch  120 | Loss 0.50800 | Correct   49 | Time 0.2891s\n",
      "Epoch  130 | Loss 0.00265 | Correct   49 | Time 0.1766s\n",
      "Epoch  140 | Loss 0.33397 | Correct   49 | Time 0.1727s\n",
      "Epoch  150 | Loss 0.00118 | Correct   49 | Time 0.1790s\n",
      "Epoch  160 | Loss 0.01589 | Correct   49 | Time 0.1762s\n",
      "Epoch  170 | Loss 0.00375 | Correct   49 | Time 0.1815s\n",
      "Epoch  180 | Loss 0.15738 | Correct   49 | Time 0.1764s\n",
      "Epoch  190 | Loss 0.00025 | Correct   50 | Time 0.3237s\n",
      "Epoch  200 | Loss 1.56359 | Correct   49 | Time 0.1789s\n",
      "Epoch  210 | Loss 0.64250 | Correct   49 | Time 0.1914s\n",
      "Epoch  220 | Loss 0.01275 | Correct   50 | Time 0.1733s\n",
      "Epoch  230 | Loss 0.02600 | Correct   49 | Time 0.1755s\n",
      "Epoch  240 | Loss 0.99766 | Correct   49 | Time 0.1758s\n",
      "Epoch  250 | Loss 0.61179 | Correct   49 | Time 0.3006s\n",
      "Epoch  260 | Loss 0.00868 | Correct   49 | Time 0.1855s\n",
      "Epoch  270 | Loss 0.73053 | Correct   49 | Time 0.1759s\n",
      "Epoch  280 | Loss 0.00444 | Correct   49 | Time 0.1726s\n",
      "Epoch  290 | Loss 0.02918 | Correct   49 | Time 0.1769s\n",
      "Epoch  300 | Loss 0.03898 | Correct   49 | Time 0.1825s\n",
      "Epoch  310 | Loss 0.00093 | Correct   49 | Time 0.3083s\n",
      "Epoch  320 | Loss 0.02415 | Correct   49 | Time 0.1752s\n",
      "Epoch  330 | Loss 0.19056 | Correct   50 | Time 0.1749s\n",
      "Epoch  340 | Loss 0.04681 | Correct   50 | Time 0.1864s\n",
      "Epoch  350 | Loss 1.29667 | Correct   49 | Time 0.1738s\n",
      "Epoch  360 | Loss 0.00059 | Correct   49 | Time 0.1770s\n",
      "Epoch  370 | Loss 0.95005 | Correct   49 | Time 0.1768s\n",
      "Epoch  380 | Loss 0.11894 | Correct   49 | Time 0.2478s\n",
      "Epoch  390 | Loss 0.16232 | Correct   50 | Time 0.1749s\n",
      "Epoch  400 | Loss 0.00551 | Correct   49 | Time 0.1784s\n",
      "Epoch  410 | Loss 0.02392 | Correct   49 | Time 0.1759s\n",
      "Epoch  420 | Loss 0.76433 | Correct   50 | Time 0.1788s\n",
      "Epoch  430 | Loss 0.01152 | Correct   49 | Time 0.1786s\n",
      "Epoch  440 | Loss 1.01637 | Correct   49 | Time 0.3347s\n",
      "Epoch  450 | Loss 0.08019 | Correct   49 | Time 0.1706s\n",
      "Epoch  460 | Loss 0.00149 | Correct   49 | Time 0.1750s\n",
      "Epoch  470 | Loss 1.43490 | Correct   49 | Time 0.1769s\n",
      "Epoch  480 | Loss 0.00094 | Correct   50 | Time 0.1754s\n",
      "Epoch  490 | Loss 0.96715 | Correct   49 | Time 0.1726s\n",
      "Epoch  499 | Loss 0.74954 | Correct   49 | Time 0.1901s\n",
      "\n",
      "Training Statistics:\n",
      "Total training time: 120.39s\n",
      "Average epoch time: 0.2408s\n",
      "Final accuracy: 98.0% (49/50 correct)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 project/run_fast_tensor.py --BACKEND cpu --HIDDEN 200 --DATASET simple --RATE 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ktq3FRNmffhB",
    "outputId": "32338208-67b0-4f3b-c0f2-2ae944e9b8c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Loss 9.56378 | Correct   27 | Time 22.2439s\n",
      "Epoch   10 | Loss 5.87497 | Correct   38 | Time 0.0823s\n",
      "Epoch   20 | Loss 4.97132 | Correct   33 | Time 0.0818s\n",
      "Epoch   30 | Loss 5.13427 | Correct   39 | Time 0.0948s\n",
      "Epoch   40 | Loss 4.45421 | Correct   45 | Time 0.0839s\n",
      "Epoch   50 | Loss 3.60640 | Correct   45 | Time 0.0844s\n",
      "Epoch   60 | Loss 2.70161 | Correct   45 | Time 0.0834s\n",
      "Epoch   70 | Loss 2.50937 | Correct   48 | Time 0.0852s\n",
      "Epoch   80 | Loss 2.29613 | Correct   47 | Time 0.0810s\n",
      "Epoch   90 | Loss 1.47002 | Correct   49 | Time 0.0914s\n",
      "Epoch  100 | Loss 2.08763 | Correct   49 | Time 0.0827s\n",
      "Epoch  110 | Loss 1.20457 | Correct   50 | Time 0.1958s\n",
      "Epoch  120 | Loss 2.34258 | Correct   46 | Time 0.2000s\n",
      "Epoch  130 | Loss 1.73247 | Correct   50 | Time 0.0830s\n",
      "Epoch  140 | Loss 2.24137 | Correct   50 | Time 0.0819s\n",
      "Epoch  150 | Loss 1.10907 | Correct   49 | Time 0.0930s\n",
      "Epoch  160 | Loss 1.97481 | Correct   50 | Time 0.0840s\n",
      "Epoch  170 | Loss 1.51287 | Correct   49 | Time 0.0821s\n",
      "Epoch  180 | Loss 1.20732 | Correct   49 | Time 0.0820s\n",
      "Epoch  190 | Loss 0.90722 | Correct   50 | Time 0.0838s\n",
      "Epoch  200 | Loss 0.66598 | Correct   49 | Time 0.0874s\n",
      "Epoch  210 | Loss 2.32310 | Correct   49 | Time 0.0981s\n",
      "Epoch  220 | Loss 0.56872 | Correct   50 | Time 0.0824s\n",
      "Epoch  230 | Loss 1.05139 | Correct   50 | Time 0.0844s\n",
      "Epoch  240 | Loss 1.84810 | Correct   47 | Time 0.1600s\n",
      "Epoch  250 | Loss 0.39128 | Correct   50 | Time 0.1634s\n",
      "Epoch  260 | Loss 1.02614 | Correct   50 | Time 0.0845s\n",
      "Epoch  270 | Loss 0.14204 | Correct   50 | Time 0.0828s\n",
      "Epoch  280 | Loss 0.17316 | Correct   50 | Time 0.0835s\n",
      "Epoch  290 | Loss 0.40025 | Correct   50 | Time 0.0845s\n",
      "Epoch  300 | Loss 0.76673 | Correct   49 | Time 0.0864s\n",
      "Epoch  310 | Loss 0.19585 | Correct   50 | Time 0.0841s\n",
      "Epoch  320 | Loss 0.97435 | Correct   50 | Time 0.0810s\n",
      "Epoch  330 | Loss 1.18126 | Correct   48 | Time 0.0946s\n",
      "Epoch  340 | Loss 0.42291 | Correct   50 | Time 0.0838s\n",
      "Epoch  350 | Loss 0.83489 | Correct   50 | Time 0.0832s\n",
      "Epoch  360 | Loss 0.08023 | Correct   50 | Time 0.0823s\n",
      "Epoch  370 | Loss 0.42094 | Correct   50 | Time 0.0839s\n",
      "Epoch  380 | Loss 0.67600 | Correct   50 | Time 0.1907s\n",
      "Epoch  390 | Loss 0.14023 | Correct   50 | Time 0.0825s\n",
      "Epoch  400 | Loss 0.09715 | Correct   50 | Time 0.0825s\n",
      "Epoch  410 | Loss 0.32252 | Correct   50 | Time 0.0836s\n",
      "Epoch  420 | Loss 0.56743 | Correct   50 | Time 0.0845s\n",
      "Epoch  430 | Loss 0.30359 | Correct   50 | Time 0.0839s\n",
      "Epoch  440 | Loss 0.12347 | Correct   50 | Time 0.0828s\n",
      "Epoch  450 | Loss 1.02113 | Correct   49 | Time 0.0925s\n",
      "Epoch  460 | Loss 0.35889 | Correct   50 | Time 0.0826s\n",
      "Epoch  470 | Loss 0.74122 | Correct   50 | Time 0.0803s\n",
      "Epoch  480 | Loss 0.87482 | Correct   50 | Time 0.0962s\n",
      "Epoch  490 | Loss 0.30155 | Correct   49 | Time 0.0807s\n",
      "Epoch  499 | Loss 0.01597 | Correct   50 | Time 0.0969s\n",
      "\n",
      "Training Statistics:\n",
      "Total training time: 68.22s\n",
      "Average epoch time: 0.1364s\n",
      "Final accuracy: 100.0% (50/50 correct)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 project/run_fast_tensor.py --BACKEND cpu --HIDDEN 100 --DATASET split --RATE 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-M2erF1fhPu",
    "outputId": "6dbf94b1-d555-4686-9d86-8b0622bd7131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Loss 8.61004 | Correct   36 | Time 22.9050s\n",
      "Epoch   10 | Loss 3.66102 | Correct   37 | Time 0.1755s\n",
      "Epoch   20 | Loss 3.82558 | Correct   43 | Time 0.1743s\n",
      "Epoch   30 | Loss 3.88426 | Correct   44 | Time 0.1758s\n",
      "Epoch   40 | Loss 1.68393 | Correct   48 | Time 0.1789s\n",
      "Epoch   50 | Loss 1.29654 | Correct   47 | Time 0.1727s\n",
      "Epoch   60 | Loss 1.22778 | Correct   47 | Time 0.3501s\n",
      "Epoch   70 | Loss 0.99378 | Correct   47 | Time 0.1740s\n",
      "Epoch   80 | Loss 5.30669 | Correct   38 | Time 0.1865s\n",
      "Epoch   90 | Loss 1.57835 | Correct   47 | Time 0.1750s\n",
      "Epoch  100 | Loss 5.60071 | Correct   47 | Time 0.1770s\n",
      "Epoch  110 | Loss 0.55115 | Correct   47 | Time 0.1758s\n",
      "Epoch  120 | Loss 1.12870 | Correct   47 | Time 0.3169s\n",
      "Epoch  130 | Loss 0.31895 | Correct   48 | Time 0.1727s\n",
      "Epoch  140 | Loss 2.84146 | Correct   45 | Time 0.1737s\n",
      "Epoch  150 | Loss 0.98354 | Correct   50 | Time 0.1838s\n",
      "Epoch  160 | Loss 1.27085 | Correct   47 | Time 0.1831s\n",
      "Epoch  170 | Loss 0.39285 | Correct   50 | Time 0.1750s\n",
      "Epoch  180 | Loss 1.98717 | Correct   47 | Time 0.1790s\n",
      "Epoch  190 | Loss 0.36411 | Correct   47 | Time 0.2180s\n",
      "Epoch  200 | Loss 1.98961 | Correct   47 | Time 0.1840s\n",
      "Epoch  210 | Loss 0.19068 | Correct   47 | Time 0.1890s\n",
      "Epoch  220 | Loss 1.51372 | Correct   50 | Time 0.1751s\n",
      "Epoch  230 | Loss 1.13336 | Correct   50 | Time 0.1735s\n",
      "Epoch  240 | Loss 0.33377 | Correct   47 | Time 0.1822s\n",
      "Epoch  250 | Loss 0.54300 | Correct   47 | Time 0.3310s\n",
      "Epoch  260 | Loss 1.70319 | Correct   49 | Time 0.1767s\n",
      "Epoch  270 | Loss 0.60267 | Correct   49 | Time 0.1810s\n",
      "Epoch  280 | Loss 0.27865 | Correct   48 | Time 0.1787s\n",
      "Epoch  290 | Loss 0.09600 | Correct   47 | Time 0.1868s\n",
      "Epoch  300 | Loss 1.13440 | Correct   47 | Time 0.1767s\n",
      "Epoch  310 | Loss 1.54099 | Correct   47 | Time 0.3220s\n",
      "Epoch  320 | Loss 1.09288 | Correct   50 | Time 0.1738s\n",
      "Epoch  330 | Loss 1.41975 | Correct   49 | Time 0.1827s\n",
      "Epoch  340 | Loss 0.10716 | Correct   44 | Time 0.1807s\n",
      "Epoch  350 | Loss 1.11930 | Correct   49 | Time 0.1750s\n",
      "Epoch  360 | Loss 0.00597 | Correct   49 | Time 0.1745s\n",
      "Epoch  370 | Loss 0.13535 | Correct   44 | Time 0.1855s\n",
      "Epoch  380 | Loss 0.20693 | Correct   49 | Time 0.3777s\n",
      "Epoch  390 | Loss 1.44817 | Correct   47 | Time 0.1791s\n",
      "Epoch  400 | Loss 1.10614 | Correct   50 | Time 0.1740s\n",
      "Epoch  410 | Loss 0.56305 | Correct   49 | Time 0.1750s\n",
      "Epoch  420 | Loss 2.03372 | Correct   47 | Time 0.1767s\n",
      "Epoch  430 | Loss 0.56885 | Correct   49 | Time 0.1769s\n",
      "Epoch  440 | Loss 1.32153 | Correct   47 | Time 0.2553s\n",
      "Epoch  450 | Loss 0.96327 | Correct   50 | Time 0.1749s\n",
      "Epoch  460 | Loss 0.58503 | Correct   49 | Time 0.1888s\n",
      "Epoch  470 | Loss 0.17392 | Correct   49 | Time 0.1768s\n",
      "Epoch  480 | Loss 6.70144 | Correct   47 | Time 0.1763s\n",
      "Epoch  490 | Loss 0.87031 | Correct   49 | Time 0.1762s\n",
      "Epoch  499 | Loss 0.44796 | Correct   49 | Time 0.1747s\n",
      "\n",
      "Training Statistics:\n",
      "Total training time: 121.55s\n",
      "Average epoch time: 0.2431s\n",
      "Final accuracy: 98.0% (49/50 correct)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 project/run_fast_tensor.py --BACKEND cpu --HIDDEN 200 --DATASET split --RATE 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkoO288QfZY5"
   },
   "source": [
    "## Step 5: Model Training (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6nfLpjgdfcUC",
    "outputId": "94101074-6e57-422c-f683-e10c381d4dae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 100 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 16 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 8 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "Epoch    0 | Loss 6.74893 | Correct   30 | Time 4.7724s\n",
      "Epoch   10 | Loss 3.68221 | Correct   45 | Time 1.2206s\n",
      "Epoch   20 | Loss 4.27904 | Correct   45 | Time 1.2307s\n",
      "Epoch   30 | Loss 1.98498 | Correct   46 | Time 1.1980s\n",
      "Epoch   40 | Loss 2.57222 | Correct   46 | Time 1.2087s\n",
      "Epoch   50 | Loss 2.60057 | Correct   46 | Time 1.2109s\n",
      "Epoch   60 | Loss 1.96242 | Correct   48 | Time 1.9027s\n",
      "Epoch   70 | Loss 2.34030 | Correct   47 | Time 1.6595s\n",
      "Epoch   80 | Loss 4.61085 | Correct   48 | Time 1.2779s\n",
      "Epoch   90 | Loss 1.69221 | Correct   48 | Time 1.2083s\n",
      "Epoch  100 | Loss 3.01604 | Correct   48 | Time 1.2014s\n",
      "Epoch  110 | Loss 0.65313 | Correct   49 | Time 1.2104s\n",
      "Epoch  120 | Loss 2.41286 | Correct   49 | Time 1.2135s\n",
      "Epoch  130 | Loss 1.55448 | Correct   49 | Time 1.2340s\n",
      "Epoch  140 | Loss 2.06271 | Correct   49 | Time 1.1926s\n",
      "Epoch  150 | Loss 0.79334 | Correct   49 | Time 1.1870s\n",
      "Epoch  160 | Loss 1.36835 | Correct   50 | Time 1.3117s\n",
      "Epoch  170 | Loss 2.27158 | Correct   48 | Time 1.2481s\n",
      "Epoch  180 | Loss 1.42722 | Correct   50 | Time 1.2868s\n",
      "Epoch  190 | Loss 1.60353 | Correct   49 | Time 1.2142s\n",
      "Epoch  200 | Loss 0.39578 | Correct   50 | Time 1.2074s\n",
      "Epoch  210 | Loss 1.20938 | Correct   48 | Time 1.2199s\n",
      "Epoch  220 | Loss 0.31331 | Correct   50 | Time 1.1914s\n",
      "Epoch  230 | Loss 0.54305 | Correct   49 | Time 1.2030s\n",
      "Epoch  240 | Loss 0.62183 | Correct   50 | Time 1.4777s\n",
      "Epoch  250 | Loss 0.86876 | Correct   49 | Time 1.3563s\n",
      "Epoch  260 | Loss 0.78365 | Correct   50 | Time 1.5363s\n",
      "Epoch  270 | Loss 0.41404 | Correct   49 | Time 1.8799s\n",
      "Epoch  280 | Loss 0.28063 | Correct   49 | Time 1.8206s\n",
      "Epoch  290 | Loss 0.64861 | Correct   50 | Time 1.6270s\n",
      "Epoch  300 | Loss 0.55578 | Correct   49 | Time 1.3705s\n",
      "Epoch  310 | Loss 0.24771 | Correct   50 | Time 1.1837s\n",
      "Epoch  320 | Loss 0.60540 | Correct   50 | Time 1.1911s\n",
      "Epoch  330 | Loss 0.38244 | Correct   50 | Time 1.2239s\n",
      "Epoch  340 | Loss 0.12022 | Correct   50 | Time 1.1884s\n",
      "Epoch  350 | Loss 0.67449 | Correct   50 | Time 1.1887s\n",
      "Epoch  360 | Loss 0.15842 | Correct   50 | Time 1.2661s\n",
      "Epoch  370 | Loss 0.26662 | Correct   50 | Time 1.2194s\n",
      "Epoch  380 | Loss 0.19733 | Correct   50 | Time 1.1905s\n",
      "Epoch  390 | Loss 1.07970 | Correct   49 | Time 1.1984s\n",
      "Epoch  400 | Loss 0.15349 | Correct   50 | Time 1.2127s\n",
      "Epoch  410 | Loss 0.27171 | Correct   50 | Time 1.1844s\n",
      "Epoch  420 | Loss 0.76596 | Correct   50 | Time 1.2028s\n",
      "Epoch  430 | Loss 0.12659 | Correct   50 | Time 1.1931s\n",
      "Epoch  440 | Loss 0.33854 | Correct   50 | Time 1.1884s\n",
      "Epoch  450 | Loss 0.11502 | Correct   50 | Time 1.2633s\n",
      "Epoch  460 | Loss 0.26279 | Correct   50 | Time 1.1819s\n",
      "Epoch  470 | Loss 0.18283 | Correct   50 | Time 1.1949s\n",
      "Epoch  480 | Loss 0.98851 | Correct   49 | Time 1.1925s\n",
      "Epoch  490 | Loss 0.43344 | Correct   50 | Time 1.1843s\n",
      "Epoch  499 | Loss 0.85866 | Correct   49 | Time 1.1972s\n",
      "\n",
      "Training Statistics:\n",
      "Total training time: 660.95s\n",
      "Average epoch time: 1.3219s\n",
      "Final accuracy: 98.0% (49/50 correct)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 project/run_fast_tensor.py --BACKEND gpu --HIDDEN 100 --DATASET xor --RATE 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beF0pwhQifUZ",
    "outputId": "3df7e02e-e1f4-4c62-bfe9-3657996d6f61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 13 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 13 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 63 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 63 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 63 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 49 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 13 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 14 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "Epoch    0 | Loss 13.44202 | Correct   24 | Time 4.5765s\n",
      "Epoch   10 | Loss 3.66968 | Correct   38 | Time 1.2317s\n",
      "Epoch   20 | Loss 4.46238 | Correct   46 | Time 1.2235s\n",
      "Epoch   30 | Loss 2.37724 | Correct   47 | Time 1.2215s\n",
      "Epoch   40 | Loss 1.92049 | Correct   48 | Time 1.2234s\n",
      "Epoch   50 | Loss 3.48635 | Correct   45 | Time 1.3126s\n",
      "Epoch   60 | Loss 1.06995 | Correct   47 | Time 1.6527s\n",
      "Epoch   70 | Loss 1.95919 | Correct   49 | Time 1.7677s\n",
      "Epoch   80 | Loss 0.62136 | Correct   49 | Time 1.2261s\n",
      "Epoch   90 | Loss 2.19086 | Correct   49 | Time 1.2186s\n",
      "Epoch  100 | Loss 3.24818 | Correct   47 | Time 1.2209s\n",
      "Epoch  110 | Loss 0.67137 | Correct   49 | Time 1.2066s\n",
      "Epoch  120 | Loss 0.42237 | Correct   49 | Time 1.2278s\n",
      "Epoch  130 | Loss 0.74721 | Correct   49 | Time 1.2188s\n",
      "Epoch  140 | Loss 2.82544 | Correct   50 | Time 1.2788s\n",
      "Epoch  150 | Loss 1.52738 | Correct   50 | Time 1.2397s\n",
      "Epoch  160 | Loss 1.39022 | Correct   49 | Time 1.2251s\n",
      "Epoch  170 | Loss 1.06441 | Correct   49 | Time 1.2241s\n",
      "Epoch  180 | Loss 0.14619 | Correct   50 | Time 1.2126s\n",
      "Epoch  190 | Loss 2.04507 | Correct   50 | Time 1.2997s\n",
      "Epoch  200 | Loss 0.27069 | Correct   50 | Time 1.6963s\n",
      "Epoch  210 | Loss 1.00530 | Correct   48 | Time 1.4517s\n",
      "Epoch  220 | Loss 0.59552 | Correct   50 | Time 1.2146s\n",
      "Epoch  230 | Loss 0.74254 | Correct   50 | Time 1.2217s\n",
      "Epoch  240 | Loss 1.53265 | Correct   50 | Time 1.2298s\n",
      "Epoch  250 | Loss 0.38374 | Correct   50 | Time 1.2197s\n",
      "Epoch  260 | Loss 0.42751 | Correct   50 | Time 1.2190s\n",
      "Epoch  270 | Loss 0.46352 | Correct   50 | Time 1.2088s\n",
      "Epoch  280 | Loss 0.29320 | Correct   50 | Time 1.3062s\n",
      "Epoch  290 | Loss 0.17114 | Correct   50 | Time 1.2418s\n",
      "Epoch  300 | Loss 0.99148 | Correct   50 | Time 1.2148s\n",
      "Epoch  310 | Loss 0.65870 | Correct   50 | Time 1.2252s\n",
      "Epoch  320 | Loss 0.30514 | Correct   50 | Time 1.2576s\n",
      "Epoch  330 | Loss 0.78301 | Correct   50 | Time 1.6270s\n",
      "Epoch  340 | Loss 0.10664 | Correct   50 | Time 1.7501s\n",
      "Epoch  350 | Loss 0.31176 | Correct   50 | Time 1.4611s\n",
      "Epoch  360 | Loss 0.10002 | Correct   50 | Time 1.2192s\n",
      "Epoch  370 | Loss 0.30344 | Correct   50 | Time 1.2247s\n",
      "Epoch  380 | Loss 0.43722 | Correct   50 | Time 1.2360s\n",
      "Epoch  390 | Loss 0.40794 | Correct   50 | Time 1.2114s\n",
      "Epoch  400 | Loss 0.26605 | Correct   50 | Time 1.2172s\n",
      "Epoch  410 | Loss 0.52983 | Correct   50 | Time 1.2087s\n",
      "Epoch  420 | Loss 0.12157 | Correct   50 | Time 1.2804s\n",
      "Epoch  430 | Loss 0.09299 | Correct   50 | Time 1.2426s\n",
      "Epoch  440 | Loss 0.35619 | Correct   50 | Time 1.2161s\n",
      "Epoch  450 | Loss 0.36819 | Correct   50 | Time 1.2365s\n",
      "Epoch  460 | Loss 0.18236 | Correct   50 | Time 1.5191s\n",
      "Epoch  470 | Loss 0.34580 | Correct   50 | Time 1.7943s\n",
      "Epoch  480 | Loss 0.29069 | Correct   50 | Time 1.5499s\n",
      "Epoch  490 | Loss 0.24091 | Correct   50 | Time 1.2966s\n",
      "Epoch  499 | Loss 0.23400 | Correct   50 | Time 1.2183s\n",
      "\n",
      "Training Statistics:\n",
      "Total training time: 666.63s\n",
      "Average epoch time: 1.3332s\n",
      "Final accuracy: 100.0% (50/50 correct)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 project/run_fast_tensor.py --BACKEND gpu --HIDDEN 200 --DATASET xor --RATE 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7ctiFq2ihcx",
    "outputId": "22dd730a-7254-439b-cb4c-c80e1f2c8a85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 100 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 16 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 8 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "Epoch    0 | Loss 4.65883 | Correct   44 | Time 4.8649s\n",
      "Epoch   10 | Loss 2.44170 | Correct   47 | Time 1.7519s\n",
      "Epoch   20 | Loss 1.74110 | Correct   48 | Time 1.8316s\n",
      "Epoch   30 | Loss 1.37673 | Correct   49 | Time 1.4901s\n",
      "Epoch   40 | Loss 1.16213 | Correct   48 | Time 1.1852s\n",
      "Epoch   50 | Loss 0.90061 | Correct   49 | Time 1.1926s\n",
      "Epoch   60 | Loss 0.07986 | Correct   50 | Time 1.1924s\n",
      "Epoch   70 | Loss 0.51744 | Correct   50 | Time 1.2740s\n",
      "Epoch   80 | Loss 0.37480 | Correct   50 | Time 1.2265s\n",
      "Epoch   90 | Loss 0.03174 | Correct   50 | Time 1.2073s\n",
      "Epoch  100 | Loss 0.09730 | Correct   48 | Time 1.2192s\n",
      "Epoch  110 | Loss 2.02784 | Correct   49 | Time 1.2120s\n",
      "Epoch  120 | Loss 0.08251 | Correct   48 | Time 1.1933s\n",
      "Epoch  130 | Loss 1.41388 | Correct   48 | Time 1.2036s\n",
      "Epoch  140 | Loss 0.66253 | Correct   48 | Time 1.1869s\n",
      "Epoch  150 | Loss 0.27616 | Correct   50 | Time 1.2344s\n",
      "Epoch  160 | Loss 1.73283 | Correct   49 | Time 1.2774s\n",
      "Epoch  170 | Loss 1.08198 | Correct   50 | Time 1.1941s\n",
      "Epoch  180 | Loss 0.54689 | Correct   48 | Time 1.2613s\n",
      "Epoch  190 | Loss 0.96629 | Correct   48 | Time 1.3636s\n",
      "Epoch  200 | Loss 0.06676 | Correct   49 | Time 1.5693s\n",
      "Epoch  210 | Loss 0.63889 | Correct   48 | Time 1.7603s\n",
      "Epoch  220 | Loss 0.04531 | Correct   50 | Time 1.7270s\n",
      "Epoch  230 | Loss 0.63790 | Correct   48 | Time 1.4389s\n",
      "Epoch  240 | Loss 0.06695 | Correct   50 | Time 1.2014s\n",
      "Epoch  250 | Loss 0.04873 | Correct   48 | Time 1.2459s\n",
      "Epoch  260 | Loss 0.06608 | Correct   50 | Time 1.2050s\n",
      "Epoch  270 | Loss 2.34984 | Correct   48 | Time 1.2577s\n",
      "Epoch  280 | Loss 1.81686 | Correct   48 | Time 1.2178s\n",
      "Epoch  290 | Loss 0.01476 | Correct   50 | Time 1.1999s\n",
      "Epoch  300 | Loss 0.59471 | Correct   48 | Time 1.1912s\n",
      "Epoch  310 | Loss 0.01745 | Correct   48 | Time 1.2564s\n",
      "Epoch  320 | Loss 0.01696 | Correct   50 | Time 1.1994s\n",
      "Epoch  330 | Loss 0.74210 | Correct   49 | Time 1.1953s\n",
      "Epoch  340 | Loss 0.73571 | Correct   49 | Time 1.2163s\n",
      "Epoch  350 | Loss 0.01482 | Correct   50 | Time 1.1935s\n",
      "Epoch  360 | Loss 1.89407 | Correct   48 | Time 1.2518s\n",
      "Epoch  370 | Loss 1.18921 | Correct   50 | Time 1.2104s\n",
      "Epoch  380 | Loss 0.04230 | Correct   49 | Time 1.3144s\n",
      "Epoch  390 | Loss 1.89084 | Correct   48 | Time 1.5675s\n",
      "Epoch  400 | Loss 0.89328 | Correct   50 | Time 1.7987s\n",
      "Epoch  410 | Loss 0.00688 | Correct   48 | Time 1.7295s\n",
      "Epoch  420 | Loss 1.85865 | Correct   48 | Time 1.5110s\n",
      "Epoch  430 | Loss 1.00336 | Correct   50 | Time 1.1831s\n",
      "Epoch  440 | Loss 1.58248 | Correct   48 | Time 1.6801s\n",
      "Epoch  450 | Loss 0.64453 | Correct   48 | Time 1.2504s\n",
      "Epoch  460 | Loss 0.06672 | Correct   49 | Time 1.2088s\n",
      "Epoch  470 | Loss 0.00338 | Correct   49 | Time 1.2352s\n",
      "Epoch  480 | Loss 0.64781 | Correct   50 | Time 1.2305s\n",
      "Epoch  490 | Loss 0.00145 | Correct   50 | Time 1.1958s\n",
      "Epoch  499 | Loss 0.50717 | Correct   48 | Time 1.2026s\n",
      "\n",
      "Training Statistics:\n",
      "Total training time: 659.92s\n",
      "Average epoch time: 1.3198s\n",
      "Final accuracy: 96.0% (48/50 correct)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 project/run_fast_tensor.py --BACKEND gpu --HIDDEN 100 --DATASET simple --RATE 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8lLtXbiiioA",
    "outputId": "89c8aa7a-ed65-4155-b90f-d2b530a83c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 13 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 13 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 63 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 63 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 63 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 49 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 13 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 14 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "Epoch    0 | Loss 3.72224 | Correct   47 | Time 5.3742s\n",
      "Epoch   10 | Loss 0.50720 | Correct   50 | Time 1.3696s\n",
      "Epoch   20 | Loss 0.82019 | Correct   48 | Time 1.2883s\n",
      "Epoch   30 | Loss 0.47896 | Correct   50 | Time 1.2999s\n",
      "Epoch   40 | Loss 0.24574 | Correct   50 | Time 1.2886s\n",
      "Epoch   50 | Loss 0.35869 | Correct   50 | Time 1.3324s\n",
      "Epoch   60 | Loss 0.35369 | Correct   50 | Time 1.3013s\n",
      "Epoch   70 | Loss 0.15696 | Correct   50 | Time 1.3888s\n",
      "Epoch   80 | Loss 0.24112 | Correct   50 | Time 1.9121s\n",
      "Epoch   90 | Loss 0.52664 | Correct   50 | Time 1.4252s\n",
      "Epoch  100 | Loss 0.08149 | Correct   50 | Time 1.2934s\n",
      "Epoch  110 | Loss 0.50171 | Correct   50 | Time 1.2872s\n",
      "Epoch  120 | Loss 0.04966 | Correct   50 | Time 1.3114s\n",
      "Epoch  130 | Loss 0.02028 | Correct   50 | Time 1.2812s\n",
      "Epoch  140 | Loss 0.07403 | Correct   50 | Time 1.3073s\n",
      "Epoch  150 | Loss 0.70807 | Correct   50 | Time 1.6573s\n",
      "Epoch  160 | Loss 0.00190 | Correct   50 | Time 1.8090s\n",
      "Epoch  170 | Loss 0.42450 | Correct   50 | Time 1.2987s\n",
      "Epoch  180 | Loss 0.08297 | Correct   50 | Time 1.3581s\n",
      "Epoch  190 | Loss 0.35539 | Correct   50 | Time 1.3079s\n",
      "Epoch  200 | Loss 0.42327 | Correct   50 | Time 1.3310s\n",
      "Epoch  210 | Loss 0.01934 | Correct   50 | Time 1.2977s\n",
      "Epoch  220 | Loss 0.06566 | Correct   50 | Time 1.3073s\n",
      "Epoch  230 | Loss 0.00921 | Correct   50 | Time 1.7743s\n",
      "Epoch  240 | Loss 0.18919 | Correct   50 | Time 1.6101s\n",
      "Epoch  250 | Loss 0.33148 | Correct   50 | Time 1.3850s\n",
      "Epoch  260 | Loss 0.04858 | Correct   50 | Time 1.2852s\n",
      "Epoch  270 | Loss 0.32196 | Correct   50 | Time 1.3794s\n",
      "Epoch  280 | Loss 0.32450 | Correct   50 | Time 1.2972s\n",
      "Epoch  290 | Loss 0.21954 | Correct   50 | Time 1.3488s\n",
      "Epoch  300 | Loss 0.05617 | Correct   50 | Time 1.3214s\n",
      "Epoch  310 | Loss 0.32949 | Correct   50 | Time 1.7074s\n",
      "Epoch  320 | Loss 0.01212 | Correct   50 | Time 1.6165s\n",
      "Epoch  330 | Loss 0.00912 | Correct   50 | Time 1.3264s\n",
      "Epoch  340 | Loss 0.02115 | Correct   50 | Time 1.2917s\n",
      "Epoch  350 | Loss 0.20771 | Correct   50 | Time 1.2928s\n",
      "Epoch  360 | Loss 0.23029 | Correct   50 | Time 1.3825s\n",
      "Epoch  370 | Loss 0.02451 | Correct   50 | Time 1.3131s\n",
      "Epoch  380 | Loss 0.17569 | Correct   50 | Time 1.3115s\n",
      "Epoch  390 | Loss 0.18478 | Correct   50 | Time 1.9565s\n",
      "Epoch  400 | Loss 0.17559 | Correct   50 | Time 1.3564s\n",
      "Epoch  410 | Loss 0.00934 | Correct   50 | Time 1.3138s\n",
      "Epoch  420 | Loss 0.02441 | Correct   50 | Time 1.2935s\n",
      "Epoch  430 | Loss 0.00810 | Correct   50 | Time 1.2737s\n",
      "Epoch  440 | Loss 0.24150 | Correct   50 | Time 1.2812s\n",
      "Epoch  450 | Loss 0.01040 | Correct   50 | Time 1.3569s\n",
      "Epoch  460 | Loss 0.01683 | Correct   50 | Time 1.2628s\n",
      "Epoch  470 | Loss 0.00179 | Correct   50 | Time 1.7669s\n",
      "Epoch  480 | Loss 0.21615 | Correct   50 | Time 1.6265s\n",
      "Epoch  490 | Loss 0.00769 | Correct   50 | Time 1.2677s\n",
      "Epoch  499 | Loss 0.02049 | Correct   50 | Time 1.2684s\n",
      "\n",
      "Training Statistics:\n",
      "Total training time: 710.81s\n",
      "Average epoch time: 1.4216s\n",
      "Final accuracy: 100.0% (50/50 correct)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 project/run_fast_tensor.py --BACKEND gpu --HIDDEN 200 --DATASET simple --RATE 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WIRiv7f4ikXS",
    "outputId": "80fb3470-cf73-41a3-def8-87166a0c2fea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 100 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 16 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 8 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "Epoch    0 | Loss 7.61671 | Correct   30 | Time 3.9901s\n",
      "Epoch   10 | Loss 4.70640 | Correct   41 | Time 1.2290s\n",
      "Epoch   20 | Loss 3.53628 | Correct   41 | Time 1.2015s\n",
      "Epoch   30 | Loss 3.15281 | Correct   42 | Time 1.2266s\n",
      "Epoch   40 | Loss 3.70910 | Correct   40 | Time 1.1972s\n",
      "Epoch   50 | Loss 3.70521 | Correct   49 | Time 1.4571s\n",
      "Epoch   60 | Loss 2.02428 | Correct   49 | Time 1.5954s\n",
      "Epoch   70 | Loss 2.20631 | Correct   49 | Time 1.8747s\n",
      "Epoch   80 | Loss 2.43670 | Correct   50 | Time 1.7282s\n",
      "Epoch   90 | Loss 1.18299 | Correct   50 | Time 2.0738s\n",
      "Epoch  100 | Loss 1.85586 | Correct   50 | Time 1.2214s\n",
      "Epoch  110 | Loss 2.02677 | Correct   50 | Time 1.2324s\n",
      "Epoch  120 | Loss 1.85875 | Correct   50 | Time 1.2252s\n",
      "Epoch  130 | Loss 2.65483 | Correct   48 | Time 1.2188s\n",
      "Epoch  140 | Loss 1.46291 | Correct   50 | Time 1.2168s\n",
      "Epoch  150 | Loss 0.88915 | Correct   50 | Time 1.2133s\n",
      "Epoch  160 | Loss 1.53267 | Correct   50 | Time 1.2605s\n",
      "Epoch  170 | Loss 0.95299 | Correct   50 | Time 1.1914s\n",
      "Epoch  180 | Loss 1.52321 | Correct   50 | Time 1.2700s\n",
      "Epoch  190 | Loss 1.00869 | Correct   49 | Time 1.1922s\n",
      "Epoch  200 | Loss 0.96922 | Correct   50 | Time 1.2104s\n",
      "Epoch  210 | Loss 0.42963 | Correct   50 | Time 1.2053s\n",
      "Epoch  220 | Loss 0.98152 | Correct   50 | Time 1.2267s\n",
      "Epoch  230 | Loss 0.29644 | Correct   50 | Time 1.2910s\n",
      "Epoch  240 | Loss 0.28565 | Correct   50 | Time 1.5309s\n",
      "Epoch  250 | Loss 0.10307 | Correct   50 | Time 1.9246s\n",
      "Epoch  260 | Loss 0.36127 | Correct   50 | Time 1.6714s\n",
      "Epoch  270 | Loss 0.81062 | Correct   50 | Time 1.2975s\n",
      "Epoch  280 | Loss 0.53801 | Correct   50 | Time 1.2204s\n",
      "Epoch  290 | Loss 0.34542 | Correct   50 | Time 1.2480s\n",
      "Epoch  300 | Loss 0.66368 | Correct   50 | Time 1.2072s\n",
      "Epoch  310 | Loss 0.72571 | Correct   50 | Time 1.1919s\n",
      "Epoch  320 | Loss 0.72720 | Correct   50 | Time 1.2069s\n",
      "Epoch  330 | Loss 0.11968 | Correct   50 | Time 1.2468s\n",
      "Epoch  340 | Loss 0.47620 | Correct   49 | Time 1.1962s\n",
      "Epoch  350 | Loss 0.36146 | Correct   50 | Time 1.2092s\n",
      "Epoch  360 | Loss 0.67292 | Correct   50 | Time 1.2958s\n",
      "Epoch  370 | Loss 0.06553 | Correct   50 | Time 1.1793s\n",
      "Epoch  380 | Loss 0.75261 | Correct   49 | Time 1.1949s\n",
      "Epoch  390 | Loss 0.44046 | Correct   50 | Time 1.1888s\n",
      "Epoch  400 | Loss 0.05327 | Correct   50 | Time 1.2236s\n",
      "Epoch  410 | Loss 0.12084 | Correct   50 | Time 1.3430s\n",
      "Epoch  420 | Loss 0.00957 | Correct   50 | Time 1.5016s\n",
      "Epoch  430 | Loss 0.35135 | Correct   50 | Time 1.7333s\n",
      "Epoch  440 | Loss 0.10661 | Correct   50 | Time 1.4509s\n",
      "Epoch  450 | Loss 0.58598 | Correct   50 | Time 1.2990s\n",
      "Epoch  460 | Loss 0.33423 | Correct   50 | Time 1.1863s\n",
      "Epoch  470 | Loss 0.28619 | Correct   50 | Time 1.2031s\n",
      "Epoch  480 | Loss 0.27881 | Correct   50 | Time 1.2002s\n",
      "Epoch  490 | Loss 0.30167 | Correct   50 | Time 1.1902s\n",
      "Epoch  499 | Loss 0.20348 | Correct   50 | Time 1.1836s\n",
      "\n",
      "Training Statistics:\n",
      "Total training time: 661.19s\n",
      "Average epoch time: 1.3224s\n",
      "Final accuracy: 100.0% (50/50 correct)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 project/run_fast_tensor.py --BACKEND gpu --HIDDEN 100 --DATASET split --RATE 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqo-X7kvilvS",
    "outputId": "da441f0e-d7a3-4411-b53b-6ff056e9d5e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 13 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 13 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 63 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 63 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 63 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 49 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 13 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 14 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "Epoch    0 | Loss 8.07739 | Correct   34 | Time 4.3240s\n",
      "Epoch   10 | Loss 3.39391 | Correct   45 | Time 1.2882s\n",
      "Epoch   20 | Loss 5.08874 | Correct   33 | Time 1.7338s\n",
      "Epoch   30 | Loss 2.31588 | Correct   44 | Time 1.6814s\n",
      "Epoch   40 | Loss 2.94717 | Correct   49 | Time 1.2692s\n",
      "Epoch   50 | Loss 0.86988 | Correct   49 | Time 1.2621s\n",
      "Epoch   60 | Loss 0.46699 | Correct   50 | Time 1.2839s\n",
      "Epoch   70 | Loss 0.87269 | Correct   50 | Time 1.3401s\n",
      "Epoch   80 | Loss 1.34099 | Correct   49 | Time 1.2695s\n",
      "Epoch   90 | Loss 0.81256 | Correct   49 | Time 1.2860s\n",
      "Epoch  100 | Loss 0.96781 | Correct   48 | Time 1.3880s\n",
      "Epoch  110 | Loss 0.46414 | Correct   50 | Time 1.9248s\n",
      "Epoch  120 | Loss 0.32119 | Correct   48 | Time 1.3547s\n",
      "Epoch  130 | Loss 0.10924 | Correct   49 | Time 1.2612s\n",
      "Epoch  140 | Loss 1.06514 | Correct   50 | Time 1.2852s\n",
      "Epoch  150 | Loss 0.52033 | Correct   48 | Time 1.2867s\n",
      "Epoch  160 | Loss 0.22776 | Correct   50 | Time 1.3431s\n",
      "Epoch  170 | Loss 0.55374 | Correct   50 | Time 1.2694s\n",
      "Epoch  180 | Loss 0.06821 | Correct   48 | Time 1.3333s\n",
      "Epoch  190 | Loss 0.72598 | Correct   49 | Time 1.2632s\n",
      "Epoch  200 | Loss 0.72380 | Correct   48 | Time 1.7099s\n",
      "Epoch  210 | Loss 1.71427 | Correct   48 | Time 1.7890s\n",
      "Epoch  220 | Loss 0.07846 | Correct   48 | Time 1.2422s\n",
      "Epoch  230 | Loss 0.19547 | Correct   49 | Time 1.2585s\n",
      "Epoch  240 | Loss 0.13678 | Correct   50 | Time 1.2701s\n",
      "Epoch  250 | Loss 0.07230 | Correct   50 | Time 1.3581s\n",
      "Epoch  260 | Loss 0.35475 | Correct   50 | Time 1.2493s\n",
      "Epoch  270 | Loss 0.82718 | Correct   50 | Time 1.3334s\n",
      "Epoch  280 | Loss 0.57797 | Correct   50 | Time 1.2632s\n",
      "Epoch  290 | Loss 0.51121 | Correct   50 | Time 1.2665s\n",
      "Epoch  300 | Loss 0.67509 | Correct   49 | Time 1.2424s\n",
      "Epoch  310 | Loss 1.13237 | Correct   48 | Time 1.6284s\n",
      "Epoch  320 | Loss 0.15404 | Correct   49 | Time 1.7510s\n",
      "Epoch  330 | Loss 1.16018 | Correct   48 | Time 1.2534s\n",
      "Epoch  340 | Loss 0.31541 | Correct   50 | Time 1.2666s\n",
      "Epoch  350 | Loss 0.54349 | Correct   50 | Time 1.2395s\n",
      "Epoch  360 | Loss 0.18298 | Correct   50 | Time 1.3268s\n",
      "Epoch  370 | Loss 0.06131 | Correct   50 | Time 1.2869s\n",
      "Epoch  380 | Loss 0.07551 | Correct   50 | Time 1.2655s\n",
      "Epoch  390 | Loss 0.75232 | Correct   50 | Time 1.2495s\n",
      "Epoch  400 | Loss 0.02315 | Correct   50 | Time 1.2557s\n",
      "Epoch  410 | Loss 0.14731 | Correct   50 | Time 1.2921s\n",
      "Epoch  420 | Loss 0.06940 | Correct   50 | Time 1.7254s\n",
      "Epoch  430 | Loss 0.03717 | Correct   50 | Time 1.2832s\n",
      "Epoch  440 | Loss 0.14076 | Correct   50 | Time 1.2468s\n",
      "Epoch  450 | Loss 0.03433 | Correct   50 | Time 1.3321s\n",
      "Epoch  460 | Loss 0.20622 | Correct   50 | Time 1.2595s\n",
      "Epoch  470 | Loss 1.50607 | Correct   48 | Time 1.2442s\n",
      "Epoch  480 | Loss 1.29250 | Correct   50 | Time 1.2851s\n",
      "Epoch  490 | Loss 0.14493 | Correct   50 | Time 1.2501s\n",
      "Epoch  499 | Loss 0.04348 | Correct   50 | Time 1.2321s\n",
      "\n",
      "Training Statistics:\n",
      "Total training time: 689.55s\n",
      "Average epoch time: 1.3791s\n",
      "Final accuracy: 100.0% (50/50 correct)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd $DIR; PYTHONPATH=/content/$DIR python3.12 project/run_fast_tensor.py --BACKEND gpu --HIDDEN 200 --DATASET split --RATE 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSYv1pWminRE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
